{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Document Chatbot \n",
        "This is a minimal document-based chatbot using embeddings + similarity search, futher to be enhanced with better chunking and sentence-level extraction.\n",
        "\n",
        "---\n",
        "## 0. Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "d:\\Users\\Swapnil_IDS_GENAI\\GENAI\\Assignments\\projects\\Domain_specific_chatbot\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "WindowsPath('D:/Users/Swapnil_IDS_GENAI/GENAI/Assignments/projects/Domain_specific_chatbot/data')"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Importing necessary libraries\n",
        "import os\n",
        "import re # For regular expressions(data processing cleaning)\n",
        "import numpy as np\n",
        "from pathlib import Path # For handling file paths\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sentence_transformers import SentenceTransformer# For sentence embeddings\n",
        "import pdfplumber# For PDF text extraction\n",
        "import docx# \n",
        "\n",
        "DATA_DIR = Path('../data')# difining folder path for the documents to be accesed\n",
        "DATA_DIR.resolve()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d163b5fe",
      "metadata": {},
      "source": [
        "We see that the documents in the folder data will be used for answering questions and the path is also defined here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Load a document\n",
        "We'll auto-pick a document from `data/`. You can change the file name here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[WindowsPath('../data/Introduction-to-Machine-Learning.pdf'),\n",
              " WindowsPath('../data/machine_learning.docx'),\n",
              " WindowsPath('../data/machine_learning_tutotrial.txt')]"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Listing all relevant document files in the data directory\n",
        "files = [p for p in DATA_DIR.iterdir() if p.suffix.lower() in {'.pdf', '.docx', '.txt'}]\n",
        "files\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "WindowsPath('../data/Introduction-to-Machine-Learning.pdf')"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Select file (update if you want a different one)\n",
        "doc_path = files[0]\n",
        "doc_path\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b85a4532",
      "metadata": {},
      "source": [
        "we have selected the first doc for test purpose."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "132343"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Functions to read different file types\n",
        "\n",
        "# Functions to read PDF files\n",
        "def read_pdf(path):\n",
        "    text = ''\n",
        "    with pdfplumber.open(path) as pdf:\n",
        "        for page in pdf.pages:\n",
        "            page_text = page.extract_text()\n",
        "            if page_text:\n",
        "                text += page_text + '\\n'\n",
        "    return text\n",
        "\n",
        "# Function to read docx files\n",
        "def read_docx(path):\n",
        "    doc = docx.Document(path)\n",
        "    return '\\n'.join(p.text for p in doc.paragraphs)\n",
        "\n",
        "# Function to read txt files\n",
        "def read_txt(path):\n",
        "    return Path(path).read_text(encoding='utf-8', errors='ignore')\n",
        "\n",
        "# General function to load file based on its extension\n",
        "def load_file(path):\n",
        "    path = Path(path)\n",
        "    if path.suffix.lower() == '.pdf':\n",
        "        return read_pdf(path)\n",
        "    if path.suffix.lower() == '.docx':\n",
        "        return read_docx(path)\n",
        "    if path.suffix.lower() == '.txt':\n",
        "        return read_txt(path)\n",
        "    return ''\n",
        "\n",
        "text = load_file(doc_path) \n",
        "len(text)  # toltal lenghth of the document is printed\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Chunk the document\n",
        "Chunking is the step where we split a long document into smaller, meaningful pieces before we embed and search it.\n",
        "we do chunking (breaking down document content to chunks of words)by\n",
        "1.Splitting by paragraph first (keeping context intact)\n",
        "2.Splitting long paragraphs into sentences\n",
        "3.Adding overlap (last 2 sentences) to avoid cutting ideas in half\n",
        "This step is crucial because good chunking = better retrieval.\n",
        "We'll start with a basic sentence-based chunker and then improve it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(205,\n",
              " '1\\n1\\n2\\n1\\nPart 1: Introduction to Machine Learning 6\\nChapter 1: What is Machine Learning? 6\\nDefinition and History of Machine Learning 6\\nKey Concepts in Machine Learning 6\\nReal-World Applications of Machine Learning 7\\nChapter 2: Types of Machine Learning 8\\nSupervised Learning 9\\nUnsupervised Learning 9\\nReinforcement Learning 10\\nChapter 3: Applications of Machine Learning 10\\nImage and Speech Recogniti')"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Basic text chunking function based on sentence boundaries and word count\n",
        "\n",
        "def chunk_text_basic(text, max_words=300, overlap=50):\n",
        "    sentences = re.split(r'(?<=[.!?]) +', text)# Split text into sentences\n",
        "    chunks = []# List to hold the text chunks\n",
        "    chunk = []# Current chunk being built\n",
        "    word_count = 0# Current word count in the chunk\n",
        "\n",
        "    # Iterate through sentences and build chunks\n",
        "    for s in sentences:\n",
        "        words = s.split()# Split sentence into words\n",
        "        chunk.append(s)# Add sentence to current chunk\n",
        "        word_count += len(words)# Update word count\n",
        "\n",
        "        # If the chunk exceeds max_words, finalize it and start a new chunk\n",
        "        if word_count >= max_words:\n",
        "            chunks.append(' '.join(chunk))# Add the current chunk to the list of chunks\n",
        "            chunk = chunk[-max(1, overlap//10):]# Start new chunk with overlap sentences\n",
        "            word_count = sum(len(w.split()) for w in chunk)# Recalculate word count for the new chunk\n",
        "            #The result is the total word count of the chunk after overlap trimming, \n",
        "            #which keeps the chunk size accurate for the next loop.\n",
        "\n",
        "    if chunk:\n",
        "        chunks.append(' '.join(chunk))# Add any remaining sentences as the last chunk\n",
        "    return chunks\n",
        "# Chunk the loaded text\n",
        "chunks = chunk_text_basic(text)\n",
        "len(chunks), chunks[0][:400]# Show number of chunks and preview of the first chunk\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "666d8b62",
      "metadata": {},
      "source": [
        "we see that the doc has table of contents(TOC) like structure so first chunks are not so useful for answering questions because they list headings, not explanations.We can skip chunks that look like a TOC, so the chatbot only embeds real content.\n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Improved chunking (paragraph-first + sentence split)\n",
        "This keeps meaning intact and avoids cutting ideas in half."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(89,\n",
              " '1 1 2 1 Part 1: Introduction to Machine Learning 6 Chapter 1: What is Machine Learning? 6 Definition and History of Machine Learning 6 Key Concepts in Machine Learning 6 Real-World Applications of Machine Learning 7 Chapter 2: Types of Machine Learning 8 Supervised Learning 9 Unsupervised Learning 9 Reinforcement Learning 10 Chapter 3: Applications of Machine Learning 10 Image and Speech Recogniti')"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Improved text chunking function with paragraph handling and sentence overlap\n",
        "def chunk_text_better(text, max_words=250, overlap_sentences=2):\n",
        "    # Split text into paragraphs and clean up\n",
        "    paragraphs = [p.strip() for p in text.split('\\n') if p.strip()] \n",
        "    chunks = []\n",
        "    cur = []\n",
        "    cur_words = 0\n",
        "\n",
        "# Helper function to flush current chunk to the list of chunks\n",
        "    def flush_chunk():\n",
        "        if cur:\n",
        "            chunks.append(' '.join(cur))\n",
        "# Reset current chunk and word count\n",
        "    for p in paragraphs:\n",
        "        words = p.split()# Split paragraph into words\n",
        "        if len(words) > max_words:\n",
        "            # split long paragraph into sentences\n",
        "            sentences = re.split(r'(?<=[.!?]) +', p)\n",
        "            for s in sentences:\n",
        "                s_words = s.split()\n",
        "                # If adding this sentence exceeds max_words, flush current chunk\n",
        "                if cur_words + len(s_words) > max_words:\n",
        "                    flush_chunk()\n",
        "                    cur[:] = cur[-overlap_sentences:]\n",
        "                    cur_words = sum(len(x.split()) for x in cur)\n",
        "                cur.append(s)\n",
        "                cur_words += len(s_words)\n",
        "\n",
        "                # After processing all sentences, flush any remaining chunk\n",
        "        else:\n",
        "            if cur_words + len(words) > max_words:\n",
        "                flush_chunk()\n",
        "                cur[:] = cur[-overlap_sentences:]\n",
        "                cur_words = sum(len(x.split()) for x in cur)\n",
        "            cur.append(p)\n",
        "            cur_words += len(words)\n",
        "\n",
        "    flush_chunk()\n",
        "    return chunks\n",
        "# Chunk the loaded text using the improved method\n",
        "better_chunks = chunk_text_better(text)\n",
        "len(better_chunks), better_chunks[0][:400]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Embed chunks\n",
        "Lets use a small, fast model to generate embeddings.\n",
        "'sentence-transformers/all-MiniLM-L6-v2'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(89, 384)"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
        "# Encode chunks into embeddings\n",
        "chunk_embeddings = model.encode(better_chunks, show_progress_bar=False, convert_to_numpy=True)\n",
        "chunk_embeddings.shape# (num_chunks, embedding_dim)\n",
        "# This  returns the embeddings as a NumPy array (easy for math like cosine similarity).\n",
        "#chunk_embeddings becomes a 2D NumPy array, shape like:\n",
        "#So each row is the vector for one chunk, which later lets us compare questions to chunks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Retrieve top-k chunks\n",
        "This is very important to understand that a document may have hundreds of chunks and we don’t want to scan or answer from all of them.\n",
        "So we embed the question and pick the k most similar chunks.\n",
        "keeping k>1 allows us to fetch chunks that have high correlation to the question and  its supporting details. \n",
        "This  makes the top‑k retrieval the core of making the chatbot precise and fast. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[(2, 0.757), (78, 0.685), (17, 0.659)]"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Function to retrieve top-k relevant chunks for a given question\n",
        "def retrieve_chunks(question, chunks, embeddings, top_k=3):\n",
        "    # Encode the question into an embedding\n",
        "    q_emb = model.encode([question], convert_to_numpy=True)\n",
        "    # Calculate cosine similarity scores between question and chunk embeddings\n",
        "    scores = cosine_similarity(q_emb, embeddings)[0]\n",
        "    # Get indices of top-k highest scoring chunks\n",
        "    top_idx = np.argsort(scores)[-top_k:][::-1]\n",
        "    # Return list of tuples (index, score, chunk)\n",
        "    return [(i, scores[i], chunks[i]) for i in top_idx]\n",
        "\n",
        "question = 'What is machine learning?'# example question\n",
        "\n",
        "results = retrieve_chunks(question, better_chunks, chunk_embeddings)\n",
        "# Retrieve top relevant chunks\n",
        "[(i, round(score, 3)) for i, score, _ in results]# Show indices and scores of top relevant chunks\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6f791ed1",
      "metadata": {},
      "source": [
        "we see that we have fetched the relevant chunks for the query/question"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Extract most relevant sentences\n",
        "Instead of returning entire chunks, it:\n",
        "splits top chunks into sentences,embeds each sentence and picks top 2–3 most similar sentences.\n",
        "This helps return “exact relevant info”."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Machine learning is a branch of artificial intelligence that focuses on creating algorithms that can learn from data and make predictions or decisions based on that data. Machine learning algorithms are used to discover patterns and insights in data, automate decision-making processes, and create intelligent systems that can learn and adapt to new information. Definition and History of Machine Learning Machine learning has its roots in the field of statistics and has evolved over time with contributions from various fields such as computer science, mathematics, and engineering.'"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Function to extract best sentences from top chunks based on question relevance\n",
        "def extract_best_sentences(question, chunks, top_k=3, max_sentences=3):\n",
        "    # gather candidate sentences\n",
        "    candidates = []\n",
        "    for _, _, chunk in chunks:\n",
        "        \"\"\"\n",
        "        Each item in chunks is a tuple like (index, score, chunk_text)\n",
        "        We only care about the chunk text,\n",
        "        so _ ignores the index and score.\n",
        "        \"\"\"\n",
        "        sentences = [s.strip() for s in re.split(r'(?<=[.!?]) +', chunk) if len(s.strip()) > 25]\n",
        "        # Filter out very short sentences\n",
        "        candidates.extend(sentences)\n",
        "\n",
        "    if not candidates:\n",
        "        return 'No relevant sentences found.'\n",
        "\n",
        "    # embed sentences\n",
        "    sent_emb = model.encode(candidates, show_progress_bar=False, convert_to_numpy=True)\n",
        "    # we have converted all candidates to vectors(embeddings)\n",
        "    q_emb = model.encode([question], convert_to_numpy=True)# embedding for the question\n",
        "    #Calculating cosine similarity between question and candidate sentences\n",
        "    scores = cosine_similarity(q_emb, sent_emb)[0]\n",
        "    top_idx = np.argsort(scores)[-max_sentences:][::-1]# Get indices of top scoring sentences (indices) ordered from most relevant to least\n",
        "    return ' '.join([candidates[i] for i in top_idx])\n",
        "\n",
        "# Extract best sentences for the example question\n",
        "answer = extract_best_sentences(question, results)\n",
        "answer\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "16dd23fd",
      "metadata": {},
      "source": [
        "# Inference:\n",
        "\n",
        "We see that the most relevant information from the pdf is chosen to answer the question."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Wrapping into a tiny chatbot class\n",
        "This used the functions defined earlier to setup a mini chatbot with the improved logic."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'● Supervised learning involves training a model using labeled data. In supervised learning, the algorithm is trained on labeled data, where the target variable is known. Supervised learning trains a machine learning model on labeled data, where inputs and outputs are known.'"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Mini document chatbot class encapsulating ingestion and Q&A functionality\n",
        "class MiniDocChatbot:\n",
        "\n",
        "    # Initialize with a sentence transformer model\n",
        "    def __init__(self, model):\n",
        "        self.model = model # Store the model\n",
        "        self.chunks = [] # List to hold text chunks\n",
        "        self.embeddings = None\n",
        "\n",
        "\n",
        "    # Ingest text by chunking and embedding\n",
        "    def ingest(self, text):\n",
        "        self.chunks = chunk_text_better(text)# Chunk the text(our document)\n",
        "        self.embeddings = self.model.encode(self.chunks, show_progress_bar=False, convert_to_numpy=True)\n",
        "        # Embed the chunks\n",
        "\n",
        "\n",
        "    # Answer the questions based on ingested document\n",
        "    def answer(self, question, top_k=3):\n",
        "        if self.embeddings is None:\n",
        "            return 'No document loaded.'\n",
        "        # Get top relevant chunks\n",
        "        results = retrieve_chunks(question, self.chunks, self.embeddings, top_k=top_k)\n",
        "        return extract_best_sentences(question, results, max_sentences=3)\n",
        "\n",
        "bot = MiniDocChatbot(model)# create chatbot instance\n",
        "bot.ingest(text)# ingest the document text\n",
        "bot.answer('Explain supervised learning in simple terms')# Get answer to a sample question\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "776be2fc",
      "metadata": {},
      "source": [
        "We see that the best possible response is made available by the chatbot."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
